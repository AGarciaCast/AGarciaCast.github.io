@article{DBLP:journals/corr/abs-2510-08300,
 abstract = {Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization.},
 author = {Bart Kuipers and
Freek Byrman and
Daniel Uyterlinde and
Alejandro Garc√≠a-Castellanos},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2510-08300.bib},
 doi = {10.48550/ARXIV.2510.08300},
 eprint = {2510.08300},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 11 Nov 2025 13:12:19 +0100},
 title = {Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant
Graph Metanetworks},
 url = {https://doi.org/10.48550/arXiv.2510.08300},
 volume = {abs/2510.08300},
 year = {2025}
}
